{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import imageio\n",
    "imageio.plugins.ffmpeg.download()\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Declaring Gloabal Variables\n",
    "Catch_left_slope = 0\n",
    "Catch_right_slope  = 0\n",
    "Catch_left  = [0, 0, 0]\n",
    "Catch_right  = [0, 0, 0]\n",
    "vertices = [0,0,0,0]\n",
    "\n",
    "def reset_globals():\n",
    "    \"\"\"\n",
    "    Clear your globals before using new outputs\n",
    "    \"\"\"\n",
    "    print(\"RESETING GLOBAL VALUES\")\n",
    "    global Catch_left_slope \n",
    "    global Catch_right_slope \n",
    "    global Catch_left   # X1, Y1, X2\n",
    "    global Catch_right  \n",
    "    global vertices\n",
    "    Catch_left_slope  = 0\n",
    "    Catch_right_slope  = 0\n",
    "    Catch_left= [0, 0, 0]  # X1, Y1, X2\n",
    "    Catch_right = [0, 0, 0]\n",
    "    vertices = [0,0,0,0]\n",
    "    \n",
    "def draw_lines(img, lines, color=[255, 0, 0], thickness=20):\n",
    "   \n",
    "    global Catch_left_slope \n",
    "    global Catch_right_slope\n",
    "    global Catch_left\n",
    "    global Catch_right\n",
    "    \n",
    "    # DECLARE VARIABLES\n",
    "    cache_weight = 0.9\n",
    "    right_ys = []\n",
    "    right_xs = []\n",
    "    right_slopes = []\n",
    "\n",
    "    left_ys = []\n",
    "    left_xs = []\n",
    "    left_slopes = []\n",
    "    \n",
    "    midpoint = img.shape[1] / 2  # X/2\n",
    "    bottom_of_image = img.shape[0]  # Y\n",
    "    \n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:   \n",
    "            slope, intercept = np.polyfit((x1,x2), (y1,y2), 1)\n",
    "            # Seprate points, and slope, lines\n",
    "            if .35 < np.absolute(slope) <= .85: # Take all position and negative slopes\n",
    "                if slope > 0 and x1 > midpoint and x2 > midpoint:\n",
    "                    right_ys.append(y1)\n",
    "                    right_ys.append(y2)\n",
    "                    right_xs.append(x1)\n",
    "                    right_xs.append(x2)\n",
    "                    right_slopes.append(slope)\n",
    "                elif slope < 0 and x1 < midpoint and x2 < midpoint:\n",
    "                    left_ys.append(y1)\n",
    "                    left_ys.append(y2)\n",
    "                    left_xs.append(x1)\n",
    "                    left_xs.append(x2)\n",
    "                    left_slopes.append(slope)\n",
    "                    \n",
    "    # Identify and Draw Right Lane Line\n",
    "    if right_ys:\n",
    "        right_index = right_ys.index(min(right_ys))  #index of y min wil be index of x min\n",
    "        right_x1 = right_xs[right_index]\n",
    "        right_y1 = right_ys[right_index]\n",
    "        right_slope = np.median(right_slopes)\n",
    "        if Catch_right_slope != 0:\n",
    "            right_slope = right_slope + (Catch_right_slope - right_slope) * cache_weight\n",
    "\n",
    "        right_x2 = int(right_x1 + (bottom_of_image - right_y1) / right_slope)\n",
    "        \n",
    "        if Catch_right_slope != 0:\n",
    "            right_x1 = int(right_x1 + (Catch_right[0] - right_x1) * cache_weight)\n",
    "            right_y1 = int(right_y1 + (Catch_right[1] - right_y1) * cache_weight)\n",
    "            right_x2 = int(right_x2 + (Catch_right[2] - right_x2) * cache_weight)\n",
    "\n",
    "        Catch_right_slope = right_slope\n",
    "        Catch_right = [right_x1, right_y1, right_x2]\n",
    "\n",
    "        cv2.line(img, (right_x1, right_y1), (right_x2, bottom_of_image), color, thickness)\n",
    "        \n",
    "    # Identify and Draw Left Lane Line\n",
    "    if left_ys:\n",
    "        left_index = left_ys.index(min(left_ys))\n",
    "        left_x1 = left_xs[left_index]\n",
    "        left_y1 = left_ys[left_index]\n",
    "        left_slope = np.median(left_slopes)\n",
    "        if Catch_left_slope != 0:\n",
    "            left_slope = left_slope + (Catch_left_slope - left_slope) * cache_weight\n",
    "\n",
    "        left_x2 = int(left_x1 + (bottom_of_image - left_y1) / left_slope)\n",
    "\n",
    "        if Catch_left_slope != 0:\n",
    "            left_x1 = int(left_x1 + (Catch_left[0] - left_x1) * cache_weight)\n",
    "            left_y1 = int(left_y1 + (Catch_left[1] - left_y1) * cache_weight)\n",
    "            left_x2 = int(left_x2 + (Catch_left[2] - left_x2) * cache_weight)\n",
    "\n",
    "        Catch_left_slope = left_slope\n",
    "        Catch_left = [left_x1, left_y1, left_x2]\n",
    "\n",
    "        cv2.line(img, (left_x1, left_y1), (left_x2, bottom_of_image), color, thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "\n",
    "    global vertices\n",
    "     # Parameters of Process Image\n",
    "    imshape = image.shape\n",
    "    kernel_size = 3 #kernel size should be a positive odd integer\n",
    "    sigma_x = 0\n",
    "    low_canny_threshold = 65 # reject pixels below the low_threshold\n",
    "    high_canny_threshold = low_canny_threshold * 3 #detect strong edge (strong gradient) pixels above the high_threshold\n",
    "    vertices = np.array([[(0,imshape[0]), (9*imshape[1]/20, 11*imshape[0]/18), (11*imshape[1]/20, 11*imshape[0]/18), (imshape[1],imshape[0])]], dtype=np.int32) # For varibale Image Shape\n",
    "    ignore_mask_color = 255\n",
    "    rho = 1  # distance resolution in pixels of the Hough grid\n",
    "    theta = np.pi/180\n",
    "    hough_threshold = 10 # minimum number of votes (intersections in a given grid cell)\n",
    "    min_line_len = 100 # Is the minimum length of a line (in pixels) that you will accept in the output (#minimum number of pixels making up a line)\n",
    "    max_line_gap = 200 #  Is the maximum distance (in pixels) between segments that you will allow to be connected into a single line\n",
    "    α = 0.7\n",
    "    β = 1\n",
    "    λ = 0\n",
    "\n",
    "    #  Convert Color Image into Gray for processing\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "     # Apply Gaussian Blurr for removing noise from image\n",
    "    blur = cv2.GaussianBlur(gray, (kernel_size, kernel_size), sigma_x)\n",
    "\n",
    "    # Apply canny edge algorithem for detecting edges in Image\n",
    "    # The algorithm will first detect strong edge (strong gradient) pixels above the high_threshold, and reject pixels below the low_threshold. Next, pixels with values between the low_threshold and high_threshold will be included as long as they are connected to strong edges. \n",
    "    edges = cv2.Canny(blur, low_canny_threshold, high_canny_threshold)\n",
    "\n",
    "    # Select Region of Interest of Image and mask rest of it\n",
    "    mask = np.zeros_like(edges)\n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    masked = cv2.bitwise_and(edges, mask)\n",
    "\n",
    "    # Apply Hough Transform to identify and form lines of interest\n",
    "    lines = cv2.HoughLinesP(masked, rho, theta, hough_threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    hough_image = np.zeros((*masked.shape, 3), dtype=np.uint8)\n",
    "    draw_lines(hough_image, lines)\n",
    "\n",
    "    # Weight two Image and get desired output\n",
    "    result = cv2.addWeighted(image, α, hough_image, β, λ) #initial_img * α + img * β + λ\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESETING GLOBAL VALUES\n",
      "RESETING GLOBAL VALUES\n",
      "RESETING GLOBAL VALUES\n",
      "RESETING GLOBAL VALUES\n",
      "RESETING GLOBAL VALUES\n",
      "RESETING GLOBAL VALUES\n"
     ]
    }
   ],
   "source": [
    "imageNames = os.listdir('test_images/')\n",
    "for name in imageNames:\n",
    "    reset_globals()\n",
    "    image = mpimg.imread(\"test_images/{}\".format(name))\n",
    "    plt.imsave(\"final_images/final_{}\".format(name), process_image(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESETING GLOBAL VALUES\n",
      "[MoviePy] >>>> Building video white.mp4\n",
      "[MoviePy] Writing video white.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████▋| 221/222 [00:04<00:00, 47.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: white.mp4 \n",
      "\n",
      "Wall time: 5.37 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"white.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_globals()\n",
    "white_output = 'white.mp4'\n",
    "clip1 = VideoFileClip(\"solidWhiteRight.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)\n",
    "#\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESETING GLOBAL VALUES\n",
      "[MoviePy] >>>> Building video yellow.mp4\n",
      "[MoviePy] Writing video yellow.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████▉| 681/682 [00:16<00:00, 41.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: yellow.mp4 \n",
      "\n",
      "Wall time: 17.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"yellow.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_globals()\n",
    "yellow_output = 'yellow.mp4'\n",
    "clip2 = VideoFileClip('solidYellowLeft.mp4')\n",
    "yellow_clip = clip2.fl_image(process_image)\n",
    "%time yellow_clip.write_videofile(yellow_output, audio=False)\n",
    "#\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(yellow_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESETING GLOBAL VALUES\n",
      "[MoviePy] >>>> Building video white.mp4\n",
      "[MoviePy] Writing video white.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 251/251 [00:12<00:00, 19.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: white.mp4 \n",
      "\n",
      "Wall time: 14 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"extra.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_globals()\n",
    "challenge_output = 'extra.mp4'\n",
    "clip1 = VideoFileClip(\"challenge.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)\n",
    "#Play video inline\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(challenge_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflection of Assignment 1:\n",
    "\n",
    "My Lane line detection dosen't work well enough on concrete road surface where color diffrence between solid lane and road surface is less in Challenge.mp3 video. I also saw issue of detecting lane where road reflection is high. I would like to dive deep into this problem and find solution.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
